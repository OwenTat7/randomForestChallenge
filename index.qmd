---
title: "Random Forest Challenge"
subtitle: "The Power of Weak Learners"
format:
  html: default
execute:
  echo: true
  eval: true
---

# ðŸŒ² Random Forest Challenge - The Power of Weak Learners


## The Problem: Can Many Weak Learners Beat One Strong Learner?

**Core Question:** How does the number of trees in a random forest affect predictive accuracy, and how do random forests compare to simpler approaches like linear regression?

**The Challenge:** Individual decision trees are "weak learners" with limited predictive power. Random forests combine many weak trees to create a "strong learner" that generalizes better. But how many trees do we need? Do more trees always mean better performance, or is there a point of diminishing returns?

**Our Approach:** We'll compare random forests with different numbers of trees against linear regression and individual decision trees to understand the trade-offs between complexity and performance **for this dataset**.


## Data and Methodology

We analyze the Ames Housing dataset, which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is ideal for our analysis because:

- **Anticipated Non-linear Relationships:** Real estate prices have complex, non-linear relationships between features (e.g., square footage in wealthy vs. poor zip codes affects price differently)
- **Mixed Data Types:** Contains both categorical (zipCode) and numerical variables
- **Real-world Complexity:** Captures the kind of messy, real-world data where ensemble methods excel

Since we anticipate non-linear relationships, random forests are well-suited to model the relationship between features and sale price.

### Python

```{python}
#| label: load-and-model-python
#| echo: true
#| out-width: 75%

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Load data
sales_data = pd.read_csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data
model_vars = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath', 
              'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']
model_data = sales_data[model_vars].dropna()

# Convert zipCode to categorical variable - important for proper modeling
model_data['zipCode'] = model_data['zipCode'].astype('category')

# Split data
X = model_data.drop('SalePrice', axis=1)
y = model_data['SalePrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Build random forests with different numbers of trees (with corrected categorical zipCode)
rf_1 = RandomForestRegressor(n_estimators=1, max_features=3, random_state=123)
rf_5 = RandomForestRegressor(n_estimators=5, max_features=3, random_state=123)
rf_25 = RandomForestRegressor(n_estimators=25, max_features=3, random_state=123)
rf_100 = RandomForestRegressor(n_estimators=100, max_features=3, random_state=123)
rf_500 = RandomForestRegressor(n_estimators=500, max_features=3, random_state=123)
rf_1000 = RandomForestRegressor(n_estimators=1000, max_features=3, random_state=123)
rf_2000 = RandomForestRegressor(n_estimators=2000, max_features=3, random_state=123)
rf_5000 = RandomForestRegressor(n_estimators=5000, max_features=3, random_state=123)

# Fit all models
rf_1.fit(X_train, y_train)
rf_5.fit(X_train, y_train)
rf_25.fit(X_train, y_train)
rf_100.fit(X_train, y_train)
rf_500.fit(X_train, y_train)
rf_1000.fit(X_train, y_train)
rf_2000.fit(X_train, y_train)
rf_5000.fit(X_train, y_train)
```


## Results: The Power of Ensemble Learning

Our analysis reveals a clear pattern: **more trees consistently improve performance**. Let's examine the results and understand why this happens.

### Performance Trends

### Python

```{python}
#| label: performance-comparison-python
#| echo: true
#| fig-width: 10
#| fig-height: 6
#| out-width: 75%
# Calculate predictions for test data
predictions_1_test = rf_1.predict(X_test)
predictions_5_test = rf_5.predict(X_test)
predictions_25_test = rf_25.predict(X_test)
predictions_100_test = rf_100.predict(X_test)
predictions_500_test = rf_500.predict(X_test)
predictions_1000_test = rf_1000.predict(X_test)
predictions_2000_test = rf_2000.predict(X_test)
predictions_5000_test = rf_5000.predict(X_test)

# Calculate predictions for training data
predictions_1_train = rf_1.predict(X_train)
predictions_5_train = rf_5.predict(X_train)
predictions_25_train = rf_25.predict(X_train)
predictions_100_train = rf_100.predict(X_train)
predictions_500_train = rf_500.predict(X_train)
predictions_1000_train = rf_1000.predict(X_train)
predictions_2000_train = rf_2000.predict(X_train)
predictions_5000_train = rf_5000.predict(X_train)

# Calculate performance metrics for test data
rmse_1_test = np.sqrt(mean_squared_error(y_test, predictions_1_test))
rmse_5_test = np.sqrt(mean_squared_error(y_test, predictions_5_test))
rmse_25_test = np.sqrt(mean_squared_error(y_test, predictions_25_test))
rmse_100_test = np.sqrt(mean_squared_error(y_test, predictions_100_test))
rmse_500_test = np.sqrt(mean_squared_error(y_test, predictions_500_test))
rmse_1000_test = np.sqrt(mean_squared_error(y_test, predictions_1000_test))
rmse_2000_test = np.sqrt(mean_squared_error(y_test, predictions_2000_test))
rmse_5000_test = np.sqrt(mean_squared_error(y_test, predictions_5000_test))

# Calculate performance metrics for training data
rmse_1_train = np.sqrt(mean_squared_error(y_train, predictions_1_train))
rmse_5_train = np.sqrt(mean_squared_error(y_train, predictions_5_train))
rmse_25_train = np.sqrt(mean_squared_error(y_train, predictions_25_train))
rmse_100_train = np.sqrt(mean_squared_error(y_train, predictions_100_train))
rmse_500_train = np.sqrt(mean_squared_error(y_train, predictions_500_train))
rmse_1000_train = np.sqrt(mean_squared_error(y_train, predictions_1000_train))
rmse_2000_train = np.sqrt(mean_squared_error(y_train, predictions_2000_train))
rmse_5000_train = np.sqrt(mean_squared_error(y_train, predictions_5000_train))

r2_1 = r2_score(y_test, predictions_1_test)
r2_5 = r2_score(y_test, predictions_5_test)
r2_25 = r2_score(y_test, predictions_25_test)
r2_100 = r2_score(y_test, predictions_100_test)
r2_500 = r2_score(y_test, predictions_500_test)
r2_1000 = r2_score(y_test, predictions_1000_test)
r2_2000 = r2_score(y_test, predictions_2000_test)
r2_5000 = r2_score(y_test, predictions_5000_test)

# Create performance comparison
performance_data = {
    'Trees': [1, 5, 25, 100, 500, 1000, 2000, 5000],
    'RMSE_Test': [rmse_1_test, rmse_5_test, rmse_25_test, rmse_100_test, rmse_500_test, rmse_1000_test, rmse_2000_test, rmse_5000_test],
    'RMSE_Train': [rmse_1_train, rmse_5_train, rmse_25_train, rmse_100_train, rmse_500_train, rmse_1000_train, rmse_2000_train, rmse_5000_train],
    'R_squared': [r2_1, r2_5, r2_25, r2_100, r2_500, r2_1000, r2_2000, r2_5000]
}

performance_df = pd.DataFrame(performance_data)
print(performance_df)
```


***
## Student Analysis Section: The Power of More Trees {#student-analysis-section}

**Your Task:** Create visualizations and analysis to demonstrate the power of ensemble learning. You'll need to create three key components:

### 1. The Power of More Trees Visualization

**Create a visualization showing:**
- RMSE vs Number of Trees (both training and test data)
- R-squared vs Number of Trees
- Do not `echo` the code that creates the visualization

**Add Brief Discussion of the Visualization**
- Discuss where the most dramatic improvement in performance occurs as you add more trees, how dramatic is it?
- Discuss diminishing returns as you add more trees

::: {.callout-important}
## ðŸ“Š Visualization Requirements

Create two plots:
1. **RMSE Plot:** Show how RMSE decreases with more trees (both training and test)
2. **R-squared Plot:** Show how R-squared increases with more trees

Use log scale on x-axis to better show the relationship across the range of tree counts.
:::

***

## **RESPONSE: Power of More Trees Analysis**

```{python}
#| label: power-trees-visualization
#| echo: false
#| fig-width: 8
#| fig-height: 3
#| out-width: 60%

import matplotlib.pyplot as plt
import numpy as np

# Create figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))

# RMSE Plot
ax1.plot(performance_df['Trees'], performance_df['RMSE_Test'], 'o-', label='Test RMSE', linewidth=2, markersize=4)
ax1.plot(performance_df['Trees'], performance_df['RMSE_Train'], 's-', label='Training RMSE', linewidth=2, markersize=4)
ax1.set_xlabel('Number of Trees')
ax1.set_ylabel('RMSE')
ax1.set_title('RMSE vs Number of Trees')
ax1.set_xscale('log')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# R-squared Plot
ax2.plot(performance_df['Trees'], performance_df['R_squared'], 'o-', color='green', linewidth=2, markersize=4)
ax2.set_xlabel('Number of Trees')
ax2.set_ylabel('R-squared')
ax2.set_title('R-squared vs Number of Trees')
ax2.set_xscale('log')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Analysis: Power of More Trees Visualization

The ensemble learning effect is most pronounced in the early stages of tree addition. The most dramatic improvement occurs between 1 and 25 trees, where test RMSE drops from approximately $45,000 to $30,000, a 33% reduction representing the largest performance gain in the entire curve. After 100 trees, improvements become increasingly marginal, demonstrating classic diminishing returns where the first few trees provide the most benefit while additional trees offer progressively smaller gains. The stable gap between training and test performance across all tree counts indicates that random forests maintain strong generalization capabilities, explaining approximately 85% of house price variance with just 100 trees and gaining only modest improvements with thousands of additional trees.

***
### 2. Overfitting Visualization and Analysis

**Your Task:** Compare decision trees vs random forests in terms of overfitting.

**Create one visualization with two side-by-side plots showing:**
- Decision trees: How performance changes with tree complexity (max depth)
- Random forests: How performance changes with number of trees

**Your analysis should explain:**
- Why individual decision trees overfit as they become more complex
- Why random forests don't suffer from the same overfitting problem
- The mechanisms that prevent overfitting in random forests (bootstrap sampling, random feature selection, averaging)

::: {.callout-important}
## ðŸ“Š Overfitting Analysis Requirements

Create a side-by-side comparison showing:
1. **Decision Trees:** Training vs Test RMSE as max depth increases (showing overfitting)
2. **Random Forests:** Training vs Test RMSE as number of trees increases (no overfitting)

- Use the same y-axis limits for both side-by-side plots so it clearly shows whether random forests outperform decision trees.
- Do not `echo` the code that creates the visualization
:::

***

## **RESPONSE: Overfitting Analysis**

```{python}
#| label: overfitting-comparison
#| echo: false
#| fig-width: 8
#| fig-height: 3
#| out-width: 60%

from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

# Test different max depths for decision trees
max_depths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 20, 25, 30]
dt_train_rmse = []
dt_test_rmse = []

for depth in max_depths:
    dt = DecisionTreeRegressor(max_depth=depth, random_state=123)
    dt.fit(X_train, y_train)
    
    train_pred = dt.predict(X_train)
    test_pred = dt.predict(X_test)
    
    dt_train_rmse.append(np.sqrt(mean_squared_error(y_train, train_pred)))
    dt_test_rmse.append(np.sqrt(mean_squared_error(y_test, test_pred)))

# Get random forest data (already calculated)
rf_trees = [1, 5, 25, 100, 500, 1000, 2000, 5000]
rf_train_rmse = performance_df['RMSE_Train'].tolist()
rf_test_rmse = performance_df['RMSE_Test'].tolist()

# Create side-by-side comparison
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))

# Decision Trees Plot
ax1.plot(max_depths, dt_train_rmse, 'o-', label='Training RMSE', linewidth=2, markersize=4, color='blue')
ax1.plot(max_depths, dt_test_rmse, 's-', label='Test RMSE', linewidth=2, markersize=4, color='red')
ax1.set_xlabel('Max Depth')
ax1.set_ylabel('RMSE')
ax1.set_title('Decision Trees: Overfitting with Complexity')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)
ax1.set_ylim(0, 60000)  # Set consistent y-axis limits

# Random Forests Plot
ax2.plot(rf_trees, rf_train_rmse, 'o-', label='Training RMSE', linewidth=2, markersize=4, color='blue')
ax2.plot(rf_trees, rf_test_rmse, 's-', label='Test RMSE', linewidth=2, markersize=4, color='red')
ax2.set_xlabel('Number of Trees')
ax2.set_ylabel('RMSE')
ax2.set_title('Random Forests: No Overfitting with More Trees')
ax2.set_xscale('log')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)
ax2.set_ylim(0, 60000)  # Set consistent y-axis limits

plt.tight_layout()
plt.show()
```

### Analysis: Why Decision Trees Overfit While Random Forests Don't

Individual decision trees exhibit clear overfitting as complexity increases, with training RMSE dropping to near zero while test RMSE increases after depth 6, creating a growing performance gap that indicates memorization rather than learning. Random forests maintain stable performance across increasing tree counts, with training and test RMSE remaining closely aligned through three key mechanisms: bootstrap sampling creates diverse training sets that prevent memorization, random feature selection reduces correlation between trees, and averaging predictions smooths out individual tree errors while preserving the underlying signal. This overfitting resilience makes random forests preferred for real-world applications, providing consistent performance without the complexity tuning required for individual decision trees.

***
### 3. Linear Regression vs Random Forest Comparison

**Your Task:** Compare random forests to linear regression baseline.

**Create a comparison table showing:**
- Linear Regression RMSE
- Random Forest (1 tree) RMSE  
- Random Forest (100 trees) RMSE
- Random Forest (1000 trees) RMSE

**Your analysis should address:**
- The improvement in RMSE when going from 1 tree to 100 trees
- Whether switching from linear regression to 100-tree random forest shows similar improvement
- When random forests are worth the added complexity vs linear regression
- The trade-offs between interpretability and performance

::: {.callout-important}
## ðŸ“Š Comparison Requirements

Create a clear table comparing:

- Linear Regression
- Random Forest (1 tree)
- Random Forest (100 trees) 
- Random Forest (1000 trees)

Include percentage improvements over linear regression for each random forest model.
:::

***

## **RESPONSE: Linear Regression vs Random Forest Comparison**

```{python}
#| label: linear-regression-comparison
#| echo: false

from sklearn.linear_model import LinearRegression

# Fit linear regression model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Calculate linear regression predictions and RMSE
lr_train_pred = lr.predict(X_train)
lr_test_pred = lr.predict(X_test)
lr_train_rmse = np.sqrt(mean_squared_error(y_train, lr_train_pred))
lr_test_rmse = np.sqrt(mean_squared_error(y_test, lr_test_pred))

# Get random forest RMSE values (already calculated)
rf_1_test_rmse = rmse_1_test
rf_100_test_rmse = rmse_100_test
rf_1000_test_rmse = rmse_1000_test

# Calculate percentage improvements over linear regression
lr_baseline = lr_test_rmse
rf_1_improvement = ((lr_baseline - rf_1_test_rmse) / lr_baseline) * 100
rf_100_improvement = ((lr_baseline - rf_100_test_rmse) / lr_baseline) * 100
rf_1000_improvement = ((lr_baseline - rf_1000_test_rmse) / lr_baseline) * 100

# Create comparison table
comparison_data = {
    'Model': ['Linear Regression', 'Random Forest (1 tree)', 'Random Forest (100 trees)', 'Random Forest (1000 trees)'],
    'Test RMSE': [f'${lr_test_rmse:,.0f}', f'${rf_1_test_rmse:,.0f}', f'${rf_100_test_rmse:,.0f}', f'${rf_1000_test_rmse:,.0f}'],
    'Improvement over Linear Regression': ['Baseline', f'{rf_1_improvement:.1f}%', f'{rf_100_improvement:.1f}%', f'{rf_1000_improvement:.1f}%']
}

comparison_df = pd.DataFrame(comparison_data)
print("\nModel Performance Comparison:")
print(comparison_df.to_string(index=False))
```

### Analysis: Linear Regression vs Random Forest Trade-offs

The comparison reveals a surprising result: a single decision tree actually performs 43% worse than linear regression, demonstrating why individual trees are considered "weak learners." However, the power of ensemble learning becomes evident as 100-tree and 1000-tree random forests outperform linear regression by 8.4% and 9.8%, respectively. The dramatic 51% improvement from 1 to 100 trees showcases the transformative effect of combining many weak learners, while the additional 1.4% gain from 100 to 1000 trees shows diminishing returns. This analysis demonstrates why random forests are preferred over individual trees and when the added complexity over linear regression is justified - in this housing dataset with complex non-linear relationships, the 8-10% improvement makes random forests worthwhile despite their reduced interpretability.



